\documentclass[paper=a4, fontsize=11pt]{scrartcl} % A4 paper and 11pt font size

\usepackage[T1]{fontenc} % Use 8-bit encoding that has 256 glyphs
\usepackage[ngerman]{babel}
\usepackage{fourier} % Use the Adobe Utopia font for the document - comment this line to return to the LaTeX default
\usepackage{amsmath,amsfonts,amsthm} % Math packages
\usepackage{graphicx}
\usepackage[utf8]{inputenc}
\usepackage{listings}
\usepackage[section]{placeins}
\usepackage{lipsum} % Used for inserting dummy 'Lorem ipsum' text into the template
\usepackage{float}
\usepackage{multicol}

\usepackage{sectsty} % Allows customizing section commands
\allsectionsfont{\centering \normalfont\scshape} % Make all sections centered, the default font and small caps

\usepackage{fancyhdr} % Custom headers and footers

\begin{document}

\section{Bayes-Theorie}

\section{Aktivierungsfunktionen}
The activation functions should have the following properties:
\begin{itemize}
	\item continous
	\item bounded
	\item monotonically increasing
	\item differentiable
\end{itemize}
\subsection{Step Function}
\label{ssect:linear-function}
\[
\varphi(x) = \begin{cases}
1 \: \text{if} x > 0 \\
0 \: \text{if} x \leq 0 \\
\end{cases}
\]
Derivative is always 0
\subsection{Linear Function}
\label{ssect:linear-function}
\[
\varphi(x) = x
\]
Linear functins alone can only solve linear separable problems but can be used in a combination node for function approximation problems.
\subsection{Logistic / Sigmoid Function}
\label{ssect:logistic-function}
\begin{align*}
\varphi(x) = sigmoid(x) &= \frac{1}{1 + e^{-x}} \\
\frac{\delta \varphi(x)}{\varphi(x)} &= \varphi(x) (1 - \varphi(x))
\end{align*}
Good for internal nodes, bad for outpur nodes.
\begin{figure}[h]
\includegraphics[scale=0.4]{img/sigmoid}
\end{figure}

\subsection{Hyperbolic Tangent function}
\label{ssect:hyperbolic-tangent-function}
\begin{align*}
\sigma(x) = \tanh(x) &= \frac{e^x - e^{-x}}{e^x + e^{-x}} \\
\frac{\delta \sigma(x)}{\sigma(x)} = 1 - \tanh^2(x) &= 1 - \frac{(e^x - e^{-x})^2}{(e^x + e^{-x})^2}
\end{align*}
\begin{figure}[h]
\includegraphics[scale=0.6]{img/tanh}
\end{figure}
If the input has a mean of 0 then so will the output

\subsection{Softmax Function}
\label{ssect:softmax-function}
\begin{align*}
\varphi(x_j) &= \frac{e^{x_j}}{\sum_k e^{x_k}} \\
\frac{\delta \varphi(x_j)}{\varphi(x_j)} = \varphi(x_j) - \varphi(x_j)^2 &= \varphi(x_j)(1 - \varphi(x_j))
\end{align*}
Outputs an a posteriori probability $p(c | x)$ and is good for classification tasks.

\subsection{Rectified Linear Unit}
\label{ssect:softmax-function}
\begin{align*}
\varphi(x) &= max(0,x) \\
\varphi^{'} &= \begin{cases}
1 \: if x > 0\\
0 \: if x \leq 0
\end{cases}
\end{align*}
Can result in sparse networks

\section{Fehlerfunktionen}

\begin{itemize}
\item $E_{MSE}(w) = \frac{1}{2} \sum\limits_{x \in X} \sum\limits_k (t^x_k - o^x_k)^2$
\item Mean-Squared-Error = $\frac{1}{N} * SSE$
\item $E_{CE}(w) = - \sum\limits_{x \in X} \sum\limits_k [t^x_k * log(o^x_k) + (1-t^x_k)*log(1-o^x_k)]$
\end{itemize}

\section{Perzeptron Lernalgorithmus}

$w_{i}^{t+1} = w_i^t + \eta (t_x - o_x) x_i$

\section{Backpropagation}

$w = w - \eta \nabla_w E(x,w)$ mit $\nabla_w E = \frac{\partial E}{\partial o} \frac{\partial o}{\partial \sigma} \frac{\partial \sigma}{\partial w}$

\section{Hopfield}
\begin{align*}
x_j &= \sum_{i; i \neq j} u_i T_{ij} \\
u_i &= g(x_j) = \begin{cases}
1 \: if x \geq 0\\
-1 \: otherwise
\end{cases} \\
E &= - \frac{1}{2} \sum_j \sum_{i;i \neq j} u_i u_j T_{ij} \\
C & \approx 0.15N \\
\frac{N}{4 ln N} < C & < \frac{N}{2 ln N}
\end{align*}
The energy function assigns a numerical value to each possible state of the system (\textbf{Lyapunov Function}.

\section{Boltzmann-Maschinen}
\begin{align*}
z_i &= b_i + \sum_j s_j w_{ij}\\
p(s_i = 1) &= \frac{1}{1 + e^{-z_i}} \\
E &= - \sum_{i < j} w_{ij} s_i s_j - \sum_i b_i s_i \\
p(v) &= \frac{e^{-E(v)}}{\sum_u e^{-E(u)}}
\end{align*}
$b_i$: bias\\
$s_j$: state\\
$w_{ij}$: weight between state j and i\\
\textbf{Simulated Annealing}:
\[
p(s_i = 1) = \frac{1}{1 + e^{-\frac{z_i}{T}}}
\]
\section{Restricted-Boltzmann-Maschinen}
\begin{align*}
E(V, H) &= - \sum_{i01}^m \sum_{j=1}^F W_{ij} h_j v_i - \sum_{i=1}^m v_i a_i - \sum_{j=1}^F h_j b_j \\
p(h_j = 1 | V) &= \sigma(b_j + \sum_{j=1}^m W_{ij}v_i \\
p(v_i = 1 | H) &= \sigma(a_j + \sum_{j=1}^F W_{ij}h_j \\
\sigma &= \frac{1}{1+e^{-x}}
\end{align*}

\section{Reinforcement Learning}

Q-Learning
Bellmanngleichung
Policyfunktion
TD-Learning ---
SARSA

\section{Generalisierung}

$<\epsilon_{†est}> = <\epsilon_{train}> + 2 \cdot \sigma^2 \frac{p}{n}$ mit Varianz $\sigma$, Parameteranzahl $p$ und Anzahl an Trainingsbeispielen $n$


\section{Normalisierung}

\begin{itemize}
\item Max-Min (Rescaling): $x' = \frac{x - min(x)}{max(x) - min(x)}$
\item Standardisierung: $x' = \frac{x - \bar{x}}{\sigma}$
\item Skalierung auf Einheitslänge: $x' = \frac{x}{||x||}$
\item lückenhafte Daten: Null filling - Smoothing
\end{itemize}

\section{Regularisierung}

\begin{itemize}
\item L1 Norm: $||w||_{L1} = \sum\limits_j |w_j|$
\item L2 Norm: $||w||_{L2} = \sum\limits_j w^2_j$
\item KL-Divergenz:
\item Cross-Entropy: $E_{CE}(w) = - \sum\limits_{x \in X} \sum\limits_k [t^x_k * log(o^x_k) + (1-t^x_k)*log(1-o^x_k)]$
\item Edit-Distance: 
\item Dropout: 
\item Meiosis: 
\end{itemize}

\section{Adaptive Lernratenanpassung}

\begin{itemize}
\item AdaGrad: $w_t = w_{t-1} - \frac{\eta}{\sqrt{G_t+\epsilon}} L(x,w_{t-1})$ mit der Diagonalmatrix $G_t$, die die Beträge des Gradienten enthält und $\epsilon$: Smoothingterm, um Division durch 0 zu verhindern
\item $w_t = w_{t-1} - \frac{RMS[\Delta w]_{t-1}}{RMS[g]_t}g_t$, wobei $RMS[\Delta w]_t$ der \glqq root mean squared error\grqq\ $\sqrt{E[\Delta w^2]_t + \epsilon}$ ist.
\item RMSProp: $w_t = w_{t-1} - \frac{\eta}{\sqrt{E[g^2]_t + \epsilon}} g_t$
\end{itemize}

\section{Updates für Backprop}

\begin{itemize}
\item Momentum-Term: $\Delta w_{ij}(t) = - \eta \frac{\partial E}{\partial w_{ij}(t)} + \alpha * \Delta w_{ij}(t-1)$
\item QuickProp: 
\item WeightElimination: 
\end{itemize}

\section{Autoencoders}

Deaktivierung von Sparse Autoencoder: 

\section{Shared Weights bei TDNNs}

Formel: 

\section{Komisches Bild mit random $cos$ einfügen}

\section{LVQ}

Random Formeln hier:

\end{document}